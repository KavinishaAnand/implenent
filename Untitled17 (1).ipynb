{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87265d37-c32f-4fa4-82b1-cf0a07e295c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADVANCED CAUSAL INFERENCE: DOUBLE MACHINE LEARNING (DML)\n",
      "Treatment Effect Estimation with Robust Standard Errors\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TASK 1: DATASET SELECTION AND PREPARATION\n",
      "================================================================================\n",
      "\n",
      "Dataset Shape: (2000, 6)\n",
      "Features (Confounders): ['X1', 'X2', 'X3', 'X4']\n",
      "Treatment Variable: Treatment\n",
      "Outcome Variable: Outcome\n",
      "\n",
      "Treatment Distribution:\n",
      "Treatment\n",
      "1    1034\n",
      "0     966\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Outcome Summary Statistics:\n",
      "count    2000.000000\n",
      "mean        1.368740\n",
      "std         2.705285\n",
      "min        -7.420572\n",
      "25%        -0.501948\n",
      "50%         1.278405\n",
      "75%         3.189152\n",
      "max        10.121580\n",
      "Name: Outcome, dtype: float64\n",
      "\n",
      "================================================================================\n",
      "TASK 2: DOUBLE MACHINE LEARNING IMPLEMENTATION\n",
      "================================================================================\n",
      "\n",
      "Step 1: Estimating E[Y|X] using Random Forest...\n",
      "Step 2: Estimating E[T|X] using Gradient Boosting...\n",
      "\n",
      "Residuals calculated:\n",
      "  Mean of outcome residuals (V): 0.000203\n",
      "  Std of outcome residuals (V): 1.463501\n",
      "  Mean of treatment residuals (W): 0.002050\n",
      "  Std of treatment residuals (W): 0.487427\n",
      "\n",
      "================================================================================\n",
      "AVERAGE TREATMENT EFFECT (ATE) ESTIMATE: 1.929682\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TASK 3: ROBUST STANDARD ERRORS CALCULATION\n",
      "================================================================================\n",
      "\n",
      "Robust Standard Error: 0.051133\n",
      "95% Confidence Interval: [1.829460, 2.029903]\n",
      "Z-statistic: 37.7383\n",
      "P-value: 0.0500\n",
      "\n",
      "================================================================================\n",
      "TASK 4: CONDITIONAL ATE (CATE) AND SENSITIVITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Conditional Average Treatment Effect (CATE) Summary:\n",
      "  Mean CATE: 1.870660\n",
      "  Std CATE: 2.064530\n",
      "  Min CATE: -4.867653\n",
      "  Max CATE: 8.709644\n",
      "\n",
      "CATE Model Coefficients (how treatment effect varies with X):\n",
      "  X1: 1.785530\n",
      "  X2: 0.149101\n",
      "  X3: -1.707069\n",
      "  X4: 0.715686\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SENSITIVITY ANALYSIS: Baseline OLS Estimate (Biased)\n",
      "--------------------------------------------------------------------------------\n",
      "OLS ATE (ignoring confounders): 1.914762\n",
      "DML ATE (accounting for confounders): 1.929682\n",
      "Bias correction: 0.014919\n",
      "\n",
      "Interpretation: The difference shows the importance of controlling\n",
      "for confounders. DML provides a more robust estimate by using ML models\n",
      "to flexibly control for the confounding variables X1-X4.\n",
      "\n",
      "================================================================================\n",
      "ADDITIONAL ANALYSIS: TREATMENT EFFECT HETEROGENEITY\n",
      "================================================================================\n",
      "\n",
      "Treatment Effect Heterogeneity by X1:\n",
      "  CATE for high X1 (>0.045): 3.308197\n",
      "  CATE for low X1 (≤0.045): 0.433123\n",
      "  Difference: 2.875074\n",
      "\n",
      "Treatment Effect Heterogeneity by X3:\n",
      "  CATE when X3=1: 0.673102\n",
      "  CATE when X3=0: 2.615614\n",
      "  Difference: -1.942512\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY AND INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "KEY FINDINGS:\n",
      "\n",
      "1. AVERAGE TREATMENT EFFECT (ATE):\n",
      "   • Estimated ATE: 1.929682\n",
      "   • Robust SE: 0.051133\n",
      "   • 95% CI: [1.829460, 2.029903]\n",
      "   \n",
      "   Interpretation: On average, the treatment increases the outcome by \n",
      "   1.9297 units. This effect is statistically significant at \n",
      "   the 5% level.\n",
      "\n",
      "2. MODEL ROBUSTNESS:\n",
      "   • Random Forest was used to model E[Y|X] (outcome given confounders)\n",
      "   • Gradient Boosting was used to model E[T|X] (treatment given confounders)\n",
      "   • 5-fold cross-validation ensures out-of-sample predictions\n",
      "   • Robust standard errors account for heteroskedasticity\n",
      "\n",
      "3. COMPARISON WITH NAIVE APPROACH:\n",
      "   • OLS estimate (ignoring confounding): 1.914762\n",
      "   • DML estimate (controlling for confounding): 1.929682\n",
      "   • Difference (bias): 0.014919\n",
      "\n",
      "4. TREATMENT EFFECT HETEROGENEITY:\n",
      "   • Treatment effects vary across individuals (CATE range: \n",
      "     -4.8677 to 8.7096)\n",
      "   • This suggests that treatment effectiveness depends on individual \n",
      "     characteristics captured by X1-X4\n",
      "\n",
      "5. POLICY IMPLICATIONS:\n",
      "   • The positive ATE suggests the treatment is generally effective\n",
      "   • Heterogeneous effects indicate potential for targeted policies\n",
      "   • The robust DML methodology provides reliable causal estimates even\n",
      "     when the relationship between confounders and outcomes is complex\n",
      "\n",
      "TECHNICAL NOTES:\n",
      "* Double Machine Learning (DML) provides √n-consistent, asymptotically \n",
      "  normal estimates of the ATE even when nuisance functions (E[Y|X] and \n",
      "  E[T|X]) are estimated using flexible machine learning methods\n",
      "* Cross-fitting (cross-validation) prevents overfitting bias in the \n",
      "  final stage estimation\n",
      "* Robust standard errors are valid under heteroskedasticity and account \n",
      "  for the estimation error in the first stage\n",
      "\n",
      "================================================================================\n",
      "PROJECT COMPLETED SUCCESSFULLY\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Downloads/dml_synthetic_data.csv\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ADVANCED CAUSAL INFERENCE: DOUBLE MACHINE LEARNING (DML)\")\n",
    "print(\"Treatment Effect Estimation with Robust Standard Errors\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 1: Dataset Selection and Preparation\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 1: DATASET SELECTION AND PREPARATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define variables\n",
    "X_cols = ['X1', 'X2', 'X3', 'X4']  # Confounders\n",
    "treatment_col = 'Treatment'\n",
    "outcome_col = 'Outcome'\n",
    "\n",
    "X = df[X_cols].values\n",
    "T = df[treatment_col].values\n",
    "Y = df[outcome_col].values\n",
    "\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Features (Confounders): {X_cols}\")\n",
    "print(f\"Treatment Variable: {treatment_col}\")\n",
    "print(f\"Outcome Variable: {outcome_col}\")\n",
    "print(f\"\\nTreatment Distribution:\")\n",
    "print(df[treatment_col].value_counts())\n",
    "print(f\"\\nOutcome Summary Statistics:\")\n",
    "print(df[outcome_col].describe())\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 2: Implement DML Algorithm with 5-Fold Cross-Validation\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 2: DOUBLE MACHINE LEARNING IMPLEMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Predict outcome Y using confounders X (not treatment)\n",
    "print(\"\\nStep 1: Estimating E[Y|X] using Random Forest...\")\n",
    "model_y = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "Y_pred = cross_val_predict(model_y, X, Y, cv=5)\n",
    "\n",
    "# Step 2: Predict treatment T using confounders X\n",
    "print(\"Step 2: Estimating E[T|X] using Gradient Boosting...\")\n",
    "model_t = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "T_pred = cross_val_predict(model_t, X, T, cv=5)\n",
    "\n",
    "# Step 3: Calculate residuals\n",
    "V = Y - Y_pred  # Outcome residuals\n",
    "W = T - T_pred  # Treatment residuals\n",
    "\n",
    "print(f\"\\nResiduals calculated:\")\n",
    "print(f\"  Mean of outcome residuals (V): {V.mean():.6f}\")\n",
    "print(f\"  Std of outcome residuals (V): {V.std():.6f}\")\n",
    "print(f\"  Mean of treatment residuals (W): {W.mean():.6f}\")\n",
    "print(f\"  Std of treatment residuals (W): {W.std():.6f}\")\n",
    "\n",
    "# Step 4: Estimate ATE using residuals\n",
    "# ATE = Cov(V, W) / Var(W)\n",
    "ate_estimate = np.cov(V, W)[0, 1] / np.var(W)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"AVERAGE TREATMENT EFFECT (ATE) ESTIMATE: {ate_estimate:.6f}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 3: Calculate Robust Standard Errors\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 3: ROBUST STANDARD ERRORS CALCULATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n = len(Y)\n",
    "\n",
    "# Calculate the influence function components\n",
    "psi = V * W - ate_estimate * W**2\n",
    "sigma_squared = np.mean(psi**2)\n",
    "variance_w = np.var(W)\n",
    "\n",
    "# Robust standard error\n",
    "se_robust = np.sqrt(sigma_squared / (n * variance_w**2))\n",
    "\n",
    "# Calculate confidence interval\n",
    "z_critical = 1.96  # 95% confidence interval\n",
    "ci_lower = ate_estimate - z_critical * se_robust\n",
    "ci_upper = ate_estimate + z_critical * se_robust\n",
    "\n",
    "print(f\"\\nRobust Standard Error: {se_robust:.6f}\")\n",
    "print(f\"95% Confidence Interval: [{ci_lower:.6f}, {ci_upper:.6f}]\")\n",
    "print(f\"Z-statistic: {ate_estimate / se_robust:.4f}\")\n",
    "print(f\"P-value: {2 * (1 - 0.975 if abs(ate_estimate / se_robust) > 1.96 else 0.5):.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 4: Calculate CATE and Perform Sensitivity Analysis\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 4: CONDITIONAL ATE (CATE) AND SENSITIVITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Estimate CATE using a final stage regression\n",
    "# CATE(X) = E[Y(1) - Y(0) | X]\n",
    "# We'll use a simple linear model for interpretability\n",
    "cate_model = LinearRegression()\n",
    "cate_model.fit(X, V / (W + 1e-10))  # Avoid division by zero\n",
    "\n",
    "# Predict CATE for all observations\n",
    "cate_predictions = cate_model.predict(X)\n",
    "\n",
    "print(f\"\\nConditional Average Treatment Effect (CATE) Summary:\")\n",
    "print(f\"  Mean CATE: {cate_predictions.mean():.6f}\")\n",
    "print(f\"  Std CATE: {cate_predictions.std():.6f}\")\n",
    "print(f\"  Min CATE: {cate_predictions.min():.6f}\")\n",
    "print(f\"  Max CATE: {cate_predictions.max():.6f}\")\n",
    "\n",
    "# CATE coefficients interpretation\n",
    "print(f\"\\nCATE Model Coefficients (how treatment effect varies with X):\")\n",
    "for i, coef in enumerate(cate_model.coef_):\n",
    "    print(f\"  {X_cols[i]}: {coef:.6f}\")\n",
    "\n",
    "# Baseline comparison: Standard OLS estimate (ignoring confounding)\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SENSITIVITY ANALYSIS: Baseline OLS Estimate (Biased)\")\n",
    "print(\"-\"*80)\n",
    "ols_model = LinearRegression()\n",
    "ols_model.fit(T.reshape(-1, 1), Y)\n",
    "ols_ate = ols_model.coef_[0]\n",
    "\n",
    "print(f\"OLS ATE (ignoring confounders): {ols_ate:.6f}\")\n",
    "print(f\"DML ATE (accounting for confounders): {ate_estimate:.6f}\")\n",
    "print(f\"Bias correction: {ate_estimate - ols_ate:.6f}\")\n",
    "print(f\"\\nInterpretation: The difference shows the importance of controlling\")\n",
    "print(f\"for confounders. DML provides a more robust estimate by using ML models\")\n",
    "print(f\"to flexibly control for the confounding variables X1-X4.\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 5: Heterogeneity Analysis (Bonus)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADDITIONAL ANALYSIS: TREATMENT EFFECT HETEROGENEITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze heterogeneity by examining CATE across different subgroups\n",
    "# Split by median of X1 (continuous confounder)\n",
    "x1_median = np.median(X[:, 0])\n",
    "high_x1_mask = X[:, 0] > x1_median\n",
    "low_x1_mask = X[:, 0] <= x1_median\n",
    "\n",
    "cate_high_x1 = cate_predictions[high_x1_mask].mean()\n",
    "cate_low_x1 = cate_predictions[low_x1_mask].mean()\n",
    "\n",
    "print(f\"\\nTreatment Effect Heterogeneity by X1:\")\n",
    "print(f\"  CATE for high X1 (>{x1_median:.3f}): {cate_high_x1:.6f}\")\n",
    "print(f\"  CATE for low X1 (≤{x1_median:.3f}): {cate_low_x1:.6f}\")\n",
    "print(f\"  Difference: {cate_high_x1 - cate_low_x1:.6f}\")\n",
    "\n",
    "# Split by X3 (binary confounder)\n",
    "x3_1_mask = X[:, 2] == 1\n",
    "x3_0_mask = X[:, 2] == 0\n",
    "\n",
    "cate_x3_1 = cate_predictions[x3_1_mask].mean()\n",
    "cate_x3_0 = cate_predictions[x3_0_mask].mean()\n",
    "\n",
    "print(f\"\\nTreatment Effect Heterogeneity by X3:\")\n",
    "print(f\"  CATE when X3=1: {cate_x3_1:.6f}\")\n",
    "print(f\"  CATE when X3=0: {cate_x3_0:.6f}\")\n",
    "print(f\"  Difference: {cate_x3_1 - cate_x3_0:.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY AND INTERPRETATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY AND INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "KEY FINDINGS:\n",
    "\n",
    "1. AVERAGE TREATMENT EFFECT (ATE):\n",
    "   • Estimated ATE: {ate_estimate:.6f}\n",
    "   • Robust SE: {se_robust:.6f}\n",
    "   • 95% CI: [{ci_lower:.6f}, {ci_upper:.6f}]\n",
    "   \n",
    "   Interpretation: On average, the treatment increases the outcome by \n",
    "   {ate_estimate:.4f} units. This effect is statistically significant at \n",
    "   the 5% level.\n",
    "\n",
    "2. MODEL ROBUSTNESS:\n",
    "   • Random Forest was used to model E[Y|X] (outcome given confounders)\n",
    "   • Gradient Boosting was used to model E[T|X] (treatment given confounders)\n",
    "   • 5-fold cross-validation ensures out-of-sample predictions\n",
    "   • Robust standard errors account for heteroskedasticity\n",
    "\n",
    "3. COMPARISON WITH NAIVE APPROACH:\n",
    "   • OLS estimate (ignoring confounding): {ols_ate:.6f}\n",
    "   • DML estimate (controlling for confounding): {ate_estimate:.6f}\n",
    "   • Difference (bias): {ate_estimate - ols_ate:.6f}\n",
    "\n",
    "4. TREATMENT EFFECT HETEROGENEITY:\n",
    "   • Treatment effects vary across individuals (CATE range: \n",
    "     {cate_predictions.min():.4f} to {cate_predictions.max():.4f})\n",
    "   • This suggests that treatment effectiveness depends on individual \n",
    "     characteristics captured by X1-X4\n",
    "\n",
    "5. POLICY IMPLICATIONS:\n",
    "   • The positive ATE suggests the treatment is generally effective\n",
    "   • Heterogeneous effects indicate potential for targeted policies\n",
    "   • The robust DML methodology provides reliable causal estimates even\n",
    "     when the relationship between confounders and outcomes is complex\n",
    "\n",
    "TECHNICAL NOTES:\n",
    "* Double Machine Learning (DML) provides √n-consistent, asymptotically \n",
    "  normal estimates of the ATE even when nuisance functions (E[Y|X] and \n",
    "  E[T|X]) are estimated using flexible machine learning methods\n",
    "* Cross-fitting (cross-validation) prevents overfitting bias in the \n",
    "  final stage estimation\n",
    "* Robust standard errors are valid under heteroskedasticity and account \n",
    "  for the estimation error in the first stage\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PROJECT COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a209cbf-d1ef-41e6-b62a-b080728925e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
